{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stemming, rimozione stopwords e normalizzazione</h3>\n",
    "<p>In questo notebook proveremo ad addestrare i modelli utilizzando gli 'n-grams'. Al contrario della rappresentazione 'bag-of-words', in cui i singoli lessemi del linguaggio vengono utilizzati come features, utilizzeremo come fetures sotto sequenze di parole contigue di lunghezza n.<br>\n",
    "L'idea alla base di questo approcio è che vengano preservate maggiormente le informazioni relative al contesto in cui le singole parole vengono utilizzate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Libreria di ML per Python, oltre ad offrire innumerevoli implementazioni di modelli predittivi offre anche\n",
    "#una vasta gamma di funzionalità per il data cleaning, feature selection, feature extraction, etc...\n",
    "#Per ora impiegheremo la classe CountVectorizer per creare la 'bag of words'.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Per poter utilizzare la lista di stopwords di NLTK è necessario scaricarla!\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "\n",
    "#Libreria per generare grafici\n",
    "from bokeh.plotting import figure, show, output_notebook, output_file\n",
    "from bokeh.models import ColumnDataSource, LabelSet, FactorRange\n",
    "\n",
    "#Funzione invocata per redirigere l'output della libreria\n",
    "#Bokeh in modo da visualizzare i grafici nel notebook, se omesso i grafici sono generati sotto forma\n",
    "#di file HTML\n",
    "output_notebook()\n",
    "\n",
    "SEED = 99\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_FOLDER = os.path.join(os.getcwd(), \"../data\")\n",
    "RESULTS_FOLDER = os.path.join(DATA_FOLDER, \"results\")\n",
    "CV_RESULTS_FOLDER = os.path.join(RESULTS_FOLDER, \"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Funzioni ausiliarie</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_results_df(df, to_remove, to_format, format_, new_index_cols):\n",
    "    \"\"\"\n",
    "        Clean results dataframe contaning GridSearch error metrics.\n",
    "\n",
    "        Params :\n",
    "            df -> pandas.DataFrame : DataFrame to clean.\n",
    "            to_remove -> python.dict : dictionary of elements to remove.\n",
    "            new_index_cols -> python.list : list of columns to use as new index\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for k,v in to_remove.items():\n",
    "        if v is True:\n",
    "            df = df.drop(k, axis=1)\n",
    "        elif isinstance(v, list):\n",
    "            df = df.drop(k, 1).assign(**pd.DataFrame(df[k].values.tolist()))\n",
    "            for col in v:\n",
    "                df = df.drop(col, 1)\n",
    "    labels = [df[col] for col in new_index_cols]\n",
    "    labels = list(zip(*labels))\n",
    "    new_index = pd.MultiIndex.from_tuples(labels, names=new_index_cols)\n",
    "    df = df.set_index(new_index)\n",
    "    df = df.drop(new_index_cols, 1)\n",
    "    for col in to_format:\n",
    "        df[col] = df[col].apply(lambda x : format(x, format_))\n",
    "    return df\n",
    "\n",
    "def dump_results(df, fp):\n",
    "    df.to_csv(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lettura Dati</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp = os.path.join(os.getcwd(), \"../data/csv/train.csv\")\n",
    "train = pd.read_csv(\n",
    "    train_fp,\n",
    "    index_col=['review_id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fp = os.path.join(os.getcwd(), \"../data/csv/train.csv\")\n",
    "test = pd.read_csv(\n",
    "    test_fp,\n",
    "    index_col=['review_id']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Separazione feature-labels</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = train['text'], train['stars']\n",
    "X_test, Y_test = test['text'], test['stars']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Definizione della Pipeline</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', 'passthrough')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Griglia iperparametri</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'cv__ngram_range':[(1,2), (2,2), (3,3)],\n",
    "        'tfidf__norm' : ['l2', 'l1'],\n",
    "        'tfidf__smooth_idf' : [True, False],\n",
    "        'model':[SGDRegressor()],\n",
    "        'model__penalty':['l2'],#,'l1'],\n",
    "        'model__alpha':[0.00001, 0.0001, 0.001],\n",
    "        'model__max_iter':[3500],\n",
    "        'model__random_state':[SEED]\n",
    "    },\n",
    "    {\n",
    "        'cv__ngram_range':[(1,2), (2,2), (3,3)],\n",
    "        'tfidf__smooth_idf' : [True, False],\n",
    "        'model':[DecisionTreeRegressor()],\n",
    "        'model__splitter':['random'],\n",
    "        'model__max_depth':[70, 100, 130],\n",
    "        'model__min_samples_split':[800, 1000],\n",
    "        'model__min_samples_leaf':[400, 500, 600],\n",
    "        'model__max_features':[5000, 7000],\n",
    "        'model__random_state': [SEED]\n",
    "    },\n",
    "    {\n",
    "        'cv__ngram_range':[(1,3)],\n",
    "        'tfidf__smooth_idf' : [True, False],\n",
    "        'model':[SVR()],\n",
    "        'model__kernel':['linear', 'poly', 'rbf'],\n",
    "        'model__degree':[2, 5, 10],\n",
    "        'model__gamma':['scale', 'auto'],\n",
    "        'model__C':[1, 10, 50],\n",
    "        'model__max_iter':[3500]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piepeline_gs_cv = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    refit=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piepeline_gs_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dump risultati GridSearchCV</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = clean_results_df(\n",
    "    pd.DataFrame(piepeline_gs_cv.cv_results_),\n",
    "    to_remove={},\n",
    "    to_format=['mean_test_neg_mean_absolute_error', 'mean_test_neg_root_mean_squared_error'],\n",
    "    format_=\".8f\",\n",
    "    new_index_cols=['rank_test_neg_root_mean_squared_error', 'rank_test_neg_mean_absolute_error']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fp = os.path.join(CV_RESULTS_FOLDER, \"ngrams_cv_results.csv\")\n",
    "dump_results(results_df, results_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "costanza_env",
   "language": "python",
   "name": "costanza_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
